---
title: "RNASeqQuant: an R package for RNA-Seq quantification"
author: 
- Yulong Niu
- Ruben Garrido-Oter
package: RNASeqQuant
date: "`r Sys.Date()`"
bibliography: RSQref.bib
csl: nature.csl
header-includes:
  - \usepackage{amsmath}
  - \usepackage{mathtools}
  - \newcommand{\Pro}{\mathrm{P}}
  - \newcommand{\tildel}[1]{\widetilde{l_{#1}}}
  - \newcommand*{\diff}{\mathop{}\!\mathrm{d}}
output:
  BiocStyle::html_document:
    toc: true
    toc_float: true
    includes:
      before_body: header.html
  BiocStyle::pdf_document:
    toc: true
    latex_engine: pdflatex
    includes:
      in_header: header.tex
vignette: >
  %\usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{RNASeqQuant: an R package for RNA-Seq quantification}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r style, echo=FALSE, results='asis', message=FALSE}
options(tinytex.verbose = TRUE)
BiocStyle::markdown()
knitr::opts_chunk$set(tidy = FALSE,
                      warning = FALSE,
                      message = FALSE)
```

``` {r macro, echo=FALSE, results='hide', message=FALSE}
RNASeqQuant <- function() {"*[RNASeqQuant](https://github.com/YulongNiu/RNASeqQuant)*"}
Robject <- function(x){sub('%obj%', x, '<span style="background-color:#F0F0F0;color:#404040;font-family:\'Lucida Console\', monospace">%obj%</span>')}
Rclass <- function(x){sub('%obj%', x, '<span style="font-family:\'Times New Roman\', Times, serif;font-style: italic">%obj%</span>')}
```

# Overview

`r RNASeqQuant()` implements both expectation maximization and gradient descent for RNA-Seq quantification.

# Models

## EM model for single species

In single species, if $T=\{t_1, t_2, \dots, t_K\}$ is the set of transcripts (with length $l_k$ for $t_k$) and the copy number of $t_k$ is $c_k$, we define $\rho_k=\frac{c_k}{\sum\limits_{t \in T}c_t} = \frac{c_k}{M}$ is the relative abundances of $t_i$, where $M$ is the total copy number, so that $\sum\limits_{t \in T} \rho_k=1$.

For a single species RNA-Seq data-set, let $F=\{f_1, f_2, \dots, f_N\}$ be the set of transcription fragments (reads) in a total number of $N=|F|$. We assume that all fragments in $F$ have the same length $m$. In $t_k$, the number of position in which the fragment can start is $\tildel{k} = l_k - m + 1$. $\tildel{k}$ is also called *effective length* [@bray2016near; @pachter2011models].

After mapping, we can observe the alignment positions of $f$, which can be mapped to several transcripts, but we do not know the exact transcript. The estimated parameters is $\alpha=\{\alpha_1, \alpha_2, \dots, \alpha_K\}$, $\sum\limits_{t \in T} \alpha_t=1$. The probability that the fragment $f$ comes from the transcripts $t_k$ is:

\begin{equation}
\begin{split}
\Pro(f \in t_k) &= \frac{\rho_k M \tildel{k}}{\sum\limits_{t \in T} \rho_t M \tildel{t}} \\
&= \frac{\rho_k \tildel{k}}{\sum\limits_{t \in T} \rho_t \tildel{t}} \\
&= \alpha_k
\end{split}
(\#eq:1)
\end{equation}

If $f$ comes from $t_k$, the probability that $f$ mapped to a certain position of $t_k$ is:

\begin{equation}
\begin{split}
\Pro(\mathrm{pos}|f \in t_k) = \frac{1}{\tildel{k}}
\end{split}
(\#eq:2)
\end{equation}

Combining \@ref(eq:1) and \@ref(eq:2), the probability that $f$ mapped to a certain position of $t_k$ is:

\begin{equation}
\begin{split}
\Pro(f \in t_k, \mathrm{pos}) = \frac{\alpha_k}{\tildel{k}}
\end{split}
(\#eq:3)
\end{equation}

The logarithm of likelihood (LL) function is:

\begin{equation}
\begin{split}
LL(\alpha) &= \sum\limits_{f \in F} \log \left(\Pro(\mathrm{pos}|\alpha)\ \right) \\
&= \sum\limits_{f \in F} \log \left( \sum_{t \in T} \Pro(\mathrm{pos}, f \in t|\alpha) \right) \\
&= \sum\limits_{f \in F} \log \left( \sum_{t \in T} y_t \frac{\alpha_t}{\tildel{t}} \right) \\
\end{split}
(\#eq:4)
\end{equation}

where $y_k$ is the $\{0, 1\}$ indicator.

We use expectationâ€“maximization (EM) algorithm to estimate $\alpha$. In the $n$ iteration, $\Pro(f \in t_i|\mathrm{pos}, \alpha^{(n)})$ equals to:

\begin{equation}
\begin{split}
\Pro(f \in t_k|\mathrm{pos}, \alpha^{(n)}) &= \frac{\Pro(f \in t_k, \mathrm{pos}|\alpha^{(n)})}{\sum\limits_{t \in T} \Pro(f \in t, \mathrm{pos}|\alpha^{(n)})} \\
&= \frac{\alpha_k^{(n)} \frac{y_k}{\tildel{k}}}{\sum\limits_{t \in T} \alpha_t^{(n)} \frac{y_t}{\tildel{t}}} \\
&= \lambda_k^{(n)}
\end{split}
(\#eq:5)
\end{equation}

where $\sum\limits_{k=1}^{K} \lambda_k = 1$.

\begin{equation}
\begin{split}
\alpha^{(n+1)} &= \argmax_\alpha \left( 
\sum_{f \in F} \sum_{t \in T} \Pro(f \in t|\mathrm{pos}, \alpha^{(n)})  \log \left( \Pro(f \in t, \mathrm{pos}|\alpha^{(n)}) \right) 
\right) \\
&= \argmax_\alpha \left( 
\sum_{f \in F} \sum_{t \in T} \lambda_t^{(n)} \log\left(
\alpha_t^{(n)} \frac{y_t}{\tildel{t}}
\right)
\right) \\
&= \argmax_\alpha \left(
\sum_{f \in F} \sum_{i \in T}  \lambda_t^{(n)} \log\left(
\alpha_t^{(n)}
\right)
\right)
\end{split}
(\#eq:6)
\end{equation}

The $n+1$ estimation of $\alpha_i$ is:

\begin{equation*}
\begin{split}
\alpha_i^{(n+1)} &= \frac{\sum\limits_{f \in F} \lambda_k^{(n)}}{\sum\limits_{t \in T} \sum\limits_{f \in F} \lambda_t^{(n)}} \\
&= \frac{\sum\limits_{f \in F} \lambda_k^{(n)}}{N}
,\ k=1, 2, \dots, K
\end{split}
\end{equation*}

## EM model for multiple species

For a mix species RNA-Seq data-set, species $A$ and $B$ for example, $U$ is the set of transcripts (with length $l_u$ for $r_u$) in species $A$ and $V$ is the set of transcripts (with length $l_v$ for $s_v$) in species $B$. Total transcripts in species $A$ and $B$ composes the set $T=\{t_1, t_2, \dots, t_{U+V}\}$. 

The relative abundances in $A$ and $B$ are $\rho_u$, $\sum\limits_{u \in U} \rho_u= 1$ and $\theta_v$, $\sum\limits_{v \in V} \theta_v= 1$, respectively. Similarly, we define:

\begin{equation}
\begin{split}
\Pro(f \in t_u) = \frac{\rho_u \tildel{u}}{\sum\limits_{u \in U} \rho_u \tildel{u}} = \alpha_u \\
\Pro(\mathrm{pos}, f \in t_u) = \frac{\alpha_u}{\tildel{u}}
\end{split}
(\#eq:7)
\end{equation}

\begin{equation}
\begin{split}
\Pro(f \in t_v) = \frac{\theta_v \tildel{v}}{\sum\limits_{v \in V} \theta_v \tildel{v}} = \beta_v \\
\Pro(\mathrm{pos}, f \in t_v) = \frac{\beta_v}{\tildel{v}}
\end{split}
(\#eq:8)
\end{equation}

The probability of single mapped fragment is:

\begin{equation}
\begin{split}
\Pro(\mathrm{pos}|\eta)  &= \Pro(\mathrm{pos}, f \in A|\eta) + \Pro(\mathrm{pos}, f \in B|\eta)\\
&= \Pro(\mathrm{pos} | f \in A, \eta) \Pro(f \in A) + \Pro(\mathrm{pos} | f \in B, \eta) \Pro(f \in B) \\
&= \sum_{u \in U} \Pro(f \in t_u, \mathrm{pos} | f \in A, \eta) \Pro(f \in A) + \sum_{v \in V} \Pro(f \in t_v, \mathrm{pos} | f \in B, \eta) \Pro(f \in B) \\
&= \sum_{u \in U} m_u \frac{\alpha_u}{\tildel{u}} \Pro(f \in A) + \sum_{v \in V} m_v \frac{\beta_v}{\tildel{v}} \Pro(f \in B) 
\end{split}
(\#eq:9)
\end{equation}

where $m_u$ and $m_v$ are $\{0, 1\}$ indicators. The estimated parameters are:

\begin{equation*}
\begin{split}
\eta=\{\alpha_1, \dots, \alpha_u, \beta_1, \dots, \beta_v, \Pro(f \in A), \Pro(f \in B)\}
\end{split}
\end{equation*}

The LL function is:

\begin{equation}
\begin{split}
LL(\eta) &= \sum\limits_{f \in F} \log \left(\Pro(\mathrm{pos}|\eta)\ \right) \\
&= \sum\limits_{f \in F} \log \left(
\sum_{u \in U} m_u \frac{\alpha_u}{\tildel{u}} \Pro(f \in A) + \sum_{v \in V} m_v \frac{\beta_v}{\tildel{v}} \Pro(f \in B)
\right)
\end{split}
(\#eq:10)
\end{equation}

Note that \@ref(eq:10) also equals to:

\begin{equation}
\begin{split}
LL(\eta) &= \sum\limits_{f \in F}  \log \left( \sum_{t \in T, S \in \{A, B\}} \left(
\Pro(\mathrm{pos} | f \in t, f \in S, \eta) \Pro(f \in t, f \in S)
\right) \right)
\end{split}
(\#eq:11)
\end{equation}

Similar to \@ref(eq:5), 

\begin{equation}
\begin{split}
\lambda_i^{(n)} &= \frac{\alpha_i^{(n)} \frac{m_i}{\tildel{i}} \Pro(f \in A)}{\sum\limits_{i=1}^{U}\alpha_i^{(n)} \frac{m_i}{\tildel{i}} \Pro(f \in A) + \sum\limits_{j=U+1}^{U+V}\beta_j^{(n)} \frac{m_j}{\tildel{j}} \Pro(f \in B)}\\
\lambda_j^{(n)} &= \frac{\beta_j^{(n)} \frac{m_j}{\tildel{j}} \Pro(f \in A)}{\sum\limits_{i=1}^{U}\alpha_i^{(n)} \frac{m_i}{\tildel{i}} \Pro(f \in A) + \sum\limits_{j=U+1}^{U+V}\beta_j^{(n)} \frac{m_j}{\tildel{j}} \Pro(f \in B)}\\
\end{split}
(\#eq:12)
\end{equation}

In $n+1$ iteration, the parameters in the expanded EM algorithm:

\begin{equation}
\begin{split}

\alpha_i^{(n+1)} &= \frac{\sum\limits_{f \in F} \lambda_i^{(n+1)}}{\sum\limits_{f \in F} \sum\limits_{i=1}^{U} \lambda_i^{(n+1)}} \\
\beta_j^{(n+1)} &= \frac{\sum\limits_{f \in F} \lambda_j^{(n+1)}}{\sum\limits_{f \in F} \sum\limits_{j=U+1}^{U+V} \lambda_j^{(n+1)}} \\
\Pro(f \in A)^{(n+1)} &= \frac{\sum\limits_{f \in F} \sum\limits_{i=1}^{U} \lambda_i^{(n+1)}}{\sum\limits_{f \in F} \sum\limits_{i=1}^{U} \lambda_i^{(n+1)} + \sum\limits_{f \in F} \sum\limits_{j=U+1}^{U+V} \lambda_j^{(n+1)}} = \frac{\sum\limits_{f \in F} \sum\limits_{i=1}^{U} \lambda_i^{(n+1)}}{N}\\
\Pro(f \in B)^{(n+1)} &= \frac{\sum\limits_{f \in F} \sum\limits_{j=U+1}^{U+V} \lambda_j^{(n+1)}}{\sum\limits_{f \in F} \sum\limits_{i=1}^{U} \lambda_i^{(n+1)} + \sum\limits_{f \in F} \sum\limits_{j=U+1}^{U+V} \lambda_j^{(n+1)}} = \frac{\sum\limits_{f \in F} \sum\limits_{j=U+1}^{U+V} \lambda_j^{(n+1)}}{N} 
\end{split}
(\#eq:11)
\end{equation}

## GD model for single species

### Active functions and gradient

The estimated parameters $\alpha=\{\alpha_1, \alpha_2, \dots, \alpha_K\}$ has restrictions: $\sum\limits_{k=1}^K \alpha_k=1$ and $\alpha_k \in [0, 1]$. We remove the restrictions by transforming $\alpha$ as:

\begin{equation*}
\begin{split}
\alpha_t &= \frac{f(x_t)}{\sum\limits_{k=1}^{K} f(x_k)} = \frac{f(x_t)}{Z} \\
\end{split}
\end{equation*}

where $x \in (-\infty, +\infty)$, $f(x) > 0$, and $f(x)$ is derivable and monotonically increasing. Then the estimated parameters are $x=\{x_1, x_2, \dots, x_K\}$, which can be estimated by the gradient descent algorithm. Candidates of $f(x)$ are:

| Name     | Equation                                          | Derivative                                | Range                    |
|----------|---------------------------------------------------|-------------------------------------------|--------------------------|
| Softmax  | $f(x)=e^x$                                        | $f'(x)=f(x)$                              | $(0,+\infty)$            |
| SoftPlus | $f(x)=\log(1+e^x)$                                | $f'(x)=\frac{1}{1+e^x}$                   | $(0,+\infty)$            |
| Softsign | $f(x)=\frac{1}{1+\lvert x \rvert} + 1$            | $f'(x)=\frac{1}{(1+\lvert x \rvert)^2}$   | $(0,2)$                  |
| Sigmoid  | $f(x)=\frac{1}{1+e^x}$                            | $f'(x)=f(x)(1-f(x))$                      | $(0,1)$                  |
| ISRU     | $f(x)=\frac{x}{\sqrt{1+ax^2}}+\frac{1}{\sqrt{a}}$ | $f'(x)=\left(\frac{1}{(1+ax)^2}\right)^3$ | $(0,\frac{2}{\sqrt{a}})$ |
| ArcTan   | $f(x)=\tan^{-1}(x)+\frac{\pi}{2}$                 | $f'(x)=\frac{1}{1+x^2}$                   | $(0,\pi)$                |
| TanH     | $f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}+1$            | $f'(x)=1-f(x)^2$                          | $(0,1)$                  |

: (\#tab:table1) Active functions for the GD[^1].

[^1]: ISUR: inverse square root unit

Thus the loss function is defined as negative logarithm of likelihood (NLL):

\begin{equation}
\begin{split}
loss(x) = NLL &= -\sum\limits_{f \in F} \log \left( \sum_{k = 1}^{K} y_k \frac{\alpha_k}{\tildel{k}} \right) \\
&=  -\sum\limits_{f \in F} \log \left( \sum_{k = 1}^{K} \frac{f(x_k)}{Z} \frac{y_k}{\tildel{k}} \right) \\
&=  -\sum\limits_{f \in F} \log \left( \sum_{k = 1}^{K} f(x_k) \frac{y_k}{\tildel{k}} - \log Z \right)
\end{split}
(\#eq:12)
\end{equation}

The derivative of $x_i$ is:

\begin{equation}
\begin{split}
\frac{\diff loss(x)}{\diff x_i} &= -\sum\limits_{f \in F} \left( 
   \frac{f'(x_i) \frac{y_i}{\tildel{i}}}{\sum_{k = 1}^{K} f(x_k) \frac{y_k}{\tildel{k}}} - \frac{f'(x_i)}{Z}
   \right)
\end{split}
(\#eq:13)
\end{equation}

The gradient is:

\begin{equation}
\begin{split}
g &= \left( \frac{\diff loss(x)}{\diff x_1}, \frac{\diff loss(x)}{\diff x_2}, \dots, \frac{\diff loss(x)}{\diff x_k}  \right)
\end{split}
(\#eq:14)
\end{equation}

### Optimization

# References
