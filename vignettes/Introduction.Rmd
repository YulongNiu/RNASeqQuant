---
title: "RNASeqQuant: an R package for RNA-Seq quantification"
author: 
- Yulong Niu
- Ruben Garrido-Oter
package: RNASeqQuant
date: "`r Sys.Date()`"
bibliography: RSQref.bib
csl: nature.csl
header-includes:
  - \usepackage{amsmath}
  - \usepackage{mathtools}
  - \newcommand{\Pro}{\mathrm{P}}
  - \newcommand{\tildel}[1]{\widetilde{l_{#1}}}
  - \newcommand*{\diff}{\mathop{}\!\mathrm{d}}
output:
  BiocStyle::html_document:
    toc: true
    toc_float: true
    includes:
      before_body: header.html
  BiocStyle::pdf_document:
    toc: true
    latex_engine: pdflatex
    includes:
      in_header: header.tex
vignette: >
  %\usepackage[utf8]{inputenc}
  %\VignetteIndexEntry{RNASeqQuant: an R package for RNA-Seq quantification}
  %\VignetteEngine{knitr::rmarkdown}
---

```{r style, echo=FALSE, results='asis', message=FALSE}
options(tinytex.verbose = TRUE)
BiocStyle::markdown()
knitr::opts_chunk$set(tidy = FALSE,
                      warning = FALSE,
                      message = FALSE)
```

``` {r macro, echo=FALSE, results='hide', message=FALSE}
RNASeqQuant <- function() {"*[RNASeqQuant](https://github.com/YulongNiu/RNASeqQuant)*"}
Robject <- function(x){sub('%obj%', x, '<span style="background-color:#F0F0F0;color:#404040;font-family:\'Lucida Console\', monospace">%obj%</span>')}
Rclass <- function(x){sub('%obj%', x, '<span style="font-family:\'Times New Roman\', Times, serif;font-style: italic">%obj%</span>')}
```

# Overview

`r RNASeqQuant()` implements both expectation maximization and gradient descent for RNA-Seq quantification.

# Models

## EM model for single species

In single species, if $T=\{t_1, t_2, \dots, t_K\}$ is the set of transcripts (with length $l_k$ for $t_k$) and the copy number of $t_k$ is $c_k$, we define $\rho_k=\frac{c_k}{\sum\limits_{t \in T}c_t} = \frac{c_k}{M}$ is the relative abundances of $t_i$, where $M$ is the total copy number, so that $\sum\limits_{t \in T} \rho_k=1$.

For a single species RNA-Seq data-set, let $F=\{f_1, f_2, \dots, f_N\}$ be the set of transcription fragments (reads) in a total number of $N=|F|$. We assume that all fragments in $F$ have the same length $m$. In $t_k$, the number of position in which the fragment can start is $\tildel{k} = l_k - m + 1$. $\tildel{k}$ is also called *effective length* [@bray2016near; @pachter2011models].

After mapping, we can observe the alignment positions of $f$, which can be mapped to several transcripts, but we do not know the exact transcript. The estimated parameters is $\alpha=\{\alpha_1, \alpha_2, \dots, \alpha_K\}$, $\sum\limits_{t \in T} \alpha_t=1$. The probability that the fragment $f$ comes from the transcripts $t_k$ is:

\begin{equation}
\begin{split}
\Pro(f \in t_k) &= \frac{\rho_k M \tildel{k}}{\sum\limits_{t \in T} \rho_t M \tildel{t}} \\
&= \frac{\rho_k \tildel{k}}{\sum\limits_{t \in T} \rho_t \tildel{t}} \\
&= \alpha_k
\end{split}
(\#eq:1)
\end{equation}

If $f$ comes from $t_k$, the probability that $f$ mapped to a certain position of $t_k$ is:

\begin{equation}
\begin{split}
\Pro(\mathrm{pos}|f \in t_k) = \frac{1}{\tildel{k}}
\end{split}
(\#eq:2)
\end{equation}

Combining \@ref(eq:1) and \@ref(eq:2), the probability that $f$ mapped to a certain position of $t_k$ is:

\begin{equation}
\begin{split}
\Pro(f \in t_k, \mathrm{pos}) = \frac{\alpha_k}{\tildel{k}}
\end{split}
(\#eq:3)
\end{equation}

The logarithm of likelihood (LL) function is:

\begin{equation}
\begin{split}
LL(\alpha) &= \sum\limits_{f \in F} \log \left(\Pro(\mathrm{pos}|\alpha)\ \right) \\
&= \sum\limits_{f \in F} \log \left( \sum_{t \in T} \Pro(\mathrm{pos}, f \in t|\alpha) \right) \\
&= \sum\limits_{f \in F} \log \left( \sum_{t \in T} y_t \frac{\alpha_t}{\tildel{t}} \right) \\
\end{split}
(\#eq:4)
\end{equation}

where $y_k$ is the $\{0, 1\}$ indicator.

We use expectationâ€“maximization (EM) algorithm to estimate $\alpha$. In the $n$ iteration, $\Pro(f \in t_i|\mathrm{pos}, \alpha^{(n)})$ equals to:

\begin{equation}
\begin{split}
\Pro(f \in t_k|\mathrm{pos}, \alpha^{(n)}) &= \frac{\Pro(f \in t_k, \mathrm{pos}|\alpha^{(n)})}{\sum\limits_{t \in T} \Pro(f \in t, \mathrm{pos}|\alpha^{(n)})} \\
&= \frac{\alpha_k^{(n)} \frac{y_k}{\tildel{k}}}{\sum\limits_{t \in T} \alpha_t^{(n)} \frac{y_t}{\tildel{t}}} \\
&= \lambda_k^{(n)}
\end{split}
(\#eq:5)
\end{equation}

where $\sum\limits_{k=1}^{K} \lambda_k = 1$.

\begin{equation}
\begin{split}
\alpha^{(n+1)} &= \argmax_\alpha \left( 
\sum_{f \in F} \sum_{t \in T} \Pro(f \in t|\mathrm{pos}, \alpha^{(n)})  \log \left( \Pro(f \in t, \mathrm{pos}|\alpha^{(n)}) \right) 
\right) \\
&= \argmax_\alpha \left( 
\sum_{f \in F} \sum_{t \in T} \lambda_t^{(n)} \log\left(
\alpha_t^{(n)} \frac{y_t}{\tildel{t}}
\right)
\right) \\
&= \argmax_\alpha \left(
\sum_{f \in F} \sum_{i \in T}  \lambda_t^{(n)} \log\left(
\alpha_t^{(n)}
\right)
\right)
\end{split}
(\#eq:6)
\end{equation}

In the $n+1$ iteration, the estimated $\alpha_k$ is:

\begin{equation*}
\begin{split}
\alpha_k^{(n+1)} &= \frac{\sum\limits_{f \in F} \lambda_k^{(n)}}{\sum\limits_{t \in T} \sum\limits_{f \in F} \lambda_t^{(n)}} \\
&= \frac{\sum\limits_{f \in F} \lambda_k^{(n)}}{N}
,\ k=1, 2, \dots, K
\end{split}
\end{equation*}

## EM model for multiple species

For a mix species RNA-Seq data-set, species $A$ and $B$ for example, $U$ is the set of transcripts (with length $l_u$ for $r_u$) in species $A$ and $V$ is the set of transcripts (with length $l_v$ for $s_v$) in species $B$. Total transcripts in species $A$ and $B$ composes the set $T=\{t_1, t_2, \dots, t_{U+V}\}$. 

The relative abundances in $A$ and $B$ are $\rho_u$, $\sum\limits_{u \in U} \rho_u= 1$ and $\theta_v$, $\sum\limits_{v \in V} \theta_v= 1$, respectively. Similarly, we define:

\begin{equation}
\begin{split}
\Pro(f \in t_u) = \frac{\rho_u \tildel{u}}{\sum\limits_{u \in U} \rho_u \tildel{u}} = \alpha_u \\
\Pro(\mathrm{pos}, f \in t_u) = \frac{\alpha_u}{\tildel{u}}
\end{split}
(\#eq:7)
\end{equation}

\begin{equation}
\begin{split}
\Pro(f \in t_v) = \frac{\theta_v \tildel{v}}{\sum\limits_{v \in V} \theta_v \tildel{v}} = \beta_v \\
\Pro(\mathrm{pos}, f \in t_v) = \frac{\beta_v}{\tildel{v}}
\end{split}
(\#eq:8)
\end{equation}

The probability of single mapped fragment is:

\begin{equation}
\begin{split}
\Pro(\mathrm{pos})  &= \Pro(\mathrm{pos}, f \in A) + \Pro(\mathrm{pos}, f \in B)\\
&= \Pro(\mathrm{pos} | f \in A) \Pro(f \in A) + \Pro(\mathrm{pos} | f \in B) \Pro(f \in B) \\
&= \sum_{u \in U} \Pro(f \in t_u, \mathrm{pos} | f \in A) \Pro(f \in A) + \sum_{v \in V} \Pro(f \in t_v, \mathrm{pos} | f \in B) \Pro(f \in B) \\
&= \sum_{u \in U} m_u \frac{\alpha_u}{\tildel{u}} \Pro(f \in A) + \sum_{v \in V} m_v \frac{\beta_v}{\tildel{v}} \Pro(f \in B) 
\end{split}
(\#eq:9)
\end{equation}

where $m_u$ and $m_v$ are $\{0, 1\}$ indicators. The estimated parameters are:

\begin{equation*}
\begin{split}
\eta=\{\alpha_1, \dots, \alpha_u, \beta_1, \dots, \beta_v, \Pro(f \in A), \Pro(f \in B)\}
\end{split}
\end{equation*}

where $\sum\limits_{u \in U} \alpha_u=1$, $\sum\limits_{v \in V} \alpha_v=1$, and $\Pro(f \in A)+\Pro(f \in B)=1$.

The LL function is:

\begin{equation}
\begin{split}
LL(\eta) &= \sum\limits_{f \in F} \log \left(\Pro(\mathrm{pos}|\eta)\ \right) \\
&= \sum\limits_{f \in F} \log \left(
\sum_{u \in U} m_u \frac{\alpha_u}{\tildel{u}} \Pro(f \in A) + \sum_{v \in V} m_v \frac{\beta_v}{\tildel{v}} \Pro(f \in B)
\right)
\end{split}
(\#eq:10)
\end{equation}

Note that \@ref(eq:10) also equals to:

\begin{equation}
\begin{split}
LL(\eta) &= \sum\limits_{f \in F}  \log \left( \sum_{t \in T, S \in \{A, B\}} \left(
\Pro(\mathrm{pos} | f \in t, f \in S, \eta) \Pro(f \in t, f \in S)
\right) \right) \\
&=  \sum\limits_{f \in F}  \log \left( \sum_{t \in T, S \in \{A, B\}} \left(
m_t \frac{1}{\tildel{t}} \Pro(f \in t, f \in S)
\right) \right)
\end{split}
(\#eq:11)
\end{equation}

Similar to \@ref(eq:5), 

\begin{equation}
\begin{split}
\Pro(f \in t_u, f \in A | \mathrm{pos}, \eta^{(n)}) &= \frac{\alpha_u^{(n)} \frac{m_u}{\tildel{u}} \Pro(f \in A)^{(n)}}{\Pro(\mathrm{pos}|\eta)}\\
 &= \lambda_u^{(n)} \\
\Pro(f \in t_v, f \in B | \mathrm{pos}, \eta^{(n)}) &= \frac{\beta_v^{(n)} \frac{m_v}{\tildel{v}} \Pro(f \in B)^{(n)}}{\Pro(\mathrm{pos}|\eta)}\\
&= \lambda_v^{(n)}
\end{split}
(\#eq:12)
\end{equation}

In $n+1$ iteration, the parameters in the expanded EM algorithm:

\begin{equation*}
\begin{split}
\alpha_u^{(n+1)} &= \frac{\sum\limits_{f \in F} \lambda_u^{(n+1)}}{\sum\limits_{f \in F} \sum\limits_{u \in U} \lambda_u^{(n+1)}} \\
\beta_v^{(n+1)} &= \frac{\sum\limits_{f \in F} \lambda_v^{(n+1)}}{\sum\limits_{f \in F} \sum\limits_{v \in V} \lambda_v^{(n+1)}} \\
\Pro(f \in A)^{(n+1)} &= \frac{\sum\limits_{f \in F} \sum\limits_{u \in U} \lambda_u^{(n+1)}}{\sum\limits_{f \in F} \sum\limits_{u \in U} \lambda_u^{(n+1)} + \sum\limits_{f \in F} \sum\limits_{v \in V} \lambda_v^{(n+1)}} = \frac{\sum\limits_{f \in F} \sum\limits_{u \in U} \lambda_u^{(n+1)}}{N}\\
\Pro(f \in B)^{(n+1)} &= \frac{\sum\limits_{f \in F} \sum\limits_{v \in V} \lambda_v^{(n+1)}}{N} 
\end{split}
\end{equation*}

## GD model for single species

### Active functions and gradient

The estimated parameters $\alpha=\{\alpha_1, \alpha_2, \dots, \alpha_K\}$ has restrictions: $\sum\limits_{t \in T} \alpha_k=1$ and $\alpha_k \in [0, 1]$. We remove the restrictions by transforming $\alpha$ as:

\begin{equation}
\begin{split}
\alpha_k &= \frac{f(x_k)}{\sum\limits_{t \in T} f(x_t)} = \frac{f(x_k)}{Z} \\
\end{split}
(\#eq:13)
\end{equation}

where $x \in (-\infty, +\infty)$, $f(x) > 0$, and $f(x)$ is derivable and monotonically increasing. Then the parameters are $x=\{x_1, x_2, \dots, x_K\}$, which can be estimated by the gradient descent (GD) algorithm. Candidates of $f(x)$ are:

| Name     | Equation                                          | Derivative                                | Range                    |
|----------|---------------------------------------------------|-------------------------------------------|--------------------------|
| Softmax  | $f(x)=e^x$                                        | $f'(x)=f(x)$                              | $(0,+\infty)$            |
| SoftPlus | $f(x)=\log(1+e^x)$                                | $f'(x)=\frac{1}{1+e^x}$                   | $(0,+\infty)$            |
| Softsign | $f(x)=\frac{1}{1+\lvert x \rvert} + 1$            | $f'(x)=\frac{1}{(1+\lvert x \rvert)^2}$   | $(0,2)$                  |
| Sigmoid  | $f(x)=\frac{1}{1+e^x}$                            | $f'(x)=f(x)(1-f(x))$                      | $(0,1)$                  |
| ISRU     | $f(x)=\frac{x}{\sqrt{1+ax^2}}+\frac{1}{\sqrt{a}}$ | $f'(x)=\left(\frac{1}{(1+ax)^2}\right)^3$ | $(0,\frac{2}{\sqrt{a}})$ |
| ArcTan   | $f(x)=\tan^{-1}(x)+\frac{\pi}{2}$                 | $f'(x)=\frac{1}{1+x^2}$                   | $(0,\pi)$                |
| TanH     | $f(x)=\frac{e^x-e^{-x}}{e^x+e^{-x}}+1$            | $f'(x)=1-f(x)^2$                          | $(0,1)$                  |

: (\#tab:table1) Active functions for the GD[^1].

[^1]: ISUR: inverse square root unit

The loss function is defined as negative logarithm of likelihood (NLL):

\begin{equation}
\begin{split}
Loss(x) = NLL(x) &= -\sum\limits_{f \in F} \log \left( \sum_{t \in T} y_t \frac{\alpha_t}{\tildel{t}} \right) \\
&=  -\sum\limits_{f \in F} \log \left( \sum_{t \in T} \frac{f(x_t)}{Z} \frac{y_t}{\tildel{t}} \right) \\
&=  -\sum\limits_{f \in F} \left( \log\sum_{t \in T} f(x_t) \frac{y_t}{\tildel{t}} - \log Z \right)
\end{split}
(\#eq:14)
\end{equation}

The derivative of $x_k$ is:

\begin{equation}
\begin{split}
\frac{\diff Loss(x)}{\diff x_k} &= -\sum\limits_{f \in F} \left( 
   \frac{f'(x_k) \frac{y_k}{\tildel{k}}}{\sum\limits_{t \in T} f(x_t) \frac{y_t}{\tildel{t}}} - \frac{f'(x_k)}{Z}
   \right)
\end{split}
(\#eq:15)
\end{equation}

The gradient is:

\begin{equation}
\begin{split}
g(x)^{(n)} &= \left( \frac{\diff Loss(x)}{\diff x_1^{(n)}}, \frac{\diff Loss(x)}{\diff x_2^{(n)}}, \dots, \frac{\diff Loss(x)}{\diff x_k^{(n)}}  \right)
\end{split}
(\#eq:16)
\end{equation}

In the $n+1$ iteration, the estimated $x$ by the vanilla GD is:

\begin{equation}
\begin{split}
x^{(n+1)} &= x^{(n)} - \eta g(x)^{(n)}
\end{split}
(\#eq:17)
\end{equation}

where $\eta$ is the learning rate, which is a small value like $10^{-3}$.

### Optimization

#### Learning rate decay

\begin{equation}
\begin{split}
\eta ^{(n)} &= \frac{\eta}{1 + decay * n} \\
\end{split}
(\#eq:18)
\end{equation}

#### Optimization algorithms

The GD optimization algorithms have been thoroughly reviewed[@ruder2016overview]. 

* Momentum

\begin{equation}
\begin{split}
v^{(n)} &= \gamma v^{(n-1)} + \eta g(x)^{(n)} \\
x^{(n+1)} &= x^{(n)} - v^{(n)}
\end{split}
(\#eq:18)
\end{equation}

* Adagrad

\begin{equation}
\begin{split}
G^{(n)} &= \sum_{i=1}^n g^2(x)^{(i)} \\
x^{(n+1)} &= x^{(n)} - \frac{\eta}{\sqrt{G^{(n)}+ \epsilon}} g(x)^{(n)}
\end{split}
(\#eq:19)
\end{equation}

* Adadelta

\begin{equation}
\begin{split}
E[g^2]^{(n)} &= \lambda E[g^2]^{(n-1)} + (1-\lambda)g^2(x)^{(n)} \\
\Delta x^{(n)} &= - \frac{\sqrt{E[\Delta x^2]^{(n-1)}+\epsilon}}{\sqrt{E[g^2]^{(n)}+\epsilon}} g^{(n)} \\
x^{(n+1)} &= x^{(n)} + \Delta x^{(n)} \\
E[\Delta x^2]^{(n+1)} &= \lambda E[\Delta x^2]^{(n)} + (1-\lambda) \Delta x^{2(n)} \\
\end{split}
(\#eq:20)
\end{equation}

* RMSProp[@tieleman2012rmsprop]

\begin{equation}
\begin{split}
E[g^2]^{(n)} &= \lambda E[g^2]^{(n-1)} + (1-\lambda)g^2(x)^{(n)} \\
x^{(n+1)} &= x^{(n)} - \frac{\eta}{\sqrt{E[g^2]^{(n)}+\epsilon}} g(x)^{(n)}
\end{split}
(\#eq:21)
\end{equation}

* Adam[@kingma2014adam]

\begin{equation}
\begin{split}
m^{(n)} &= \beta_1 m^{(n-1)} + (1-\beta_1) g(x)^{(n)} \\
v^{(n)} &= \beta_2 v^{(n-1)} + (1-\beta_2) g^2(x)^{(n)} \\
\hat{m}^{(n)} &= \frac{m^{(n)}}{1-\beta_1^n} \\
\hat{v}^{(n)} &= \frac{v^{(n)}}{1-\beta_2^n} \\
x^{(n+1)} &= x^{(n)} - \frac{\eta}{\sqrt{\hat{v}^{(n)}} + \epsilon}\hat{m}^{(n)}
\end{split}
(\#eq:22)
\end{equation}

* AdaMax[@kingma2014adam]

\begin{equation}
\begin{split}
m^{(n)} &= \beta_1 m^{(n-1)} + (1-\beta_1) g(x)^{(n)} \\
u^{(n)} &= \max \left( \beta_2 u^{(n-1)}, \left\lvert g(x)^{(n)} \right\rvert \right) \\
\hat{m}^{(n)} &= \frac{m^{(n)}}{1-\beta_1^n} \\
x^{(n+1)} &= x^{(n)} - \frac{\eta}{u^{(n)}} \hat{m}^{(n)}
\end{split}
(\#eq:23)
\end{equation}


* AMSGrad[@reddi2019convergence]

\begin{equation}
\begin{split}
m^{(n)} &= \beta_1 m^{(n-1)} + (1-\beta_1) g(x)^{(n)} \\
v^{(n)} &= \beta_2 v^{(n-1)} + (1-\beta_2) g^2(x)^{(n)} \\
\hat{v}^{(n)} &= \max \left( \hat{v}^{(n-1)}, v^{(n)} \right) \\
x^{(n+1)} &= x^{(n)} - \frac{\eta}{\sqrt{\hat{v}^{(n)}} + \epsilon} m^{(n)}
\end{split}
(\#eq:24)
\end{equation}

* Nesterov accelerated gradient (NAG)

\begin{equation}
\begin{split}
v^{(n)} &= \gamma v^{(n-1)} + \eta g(x^{(n-1)} - \gamma v^{(n-1)})^{(n)} \\
x^{(n+1)} &= x^{(n)} - v^{(n)}
\end{split}
(\#eq:23)
\end{equation}

#### Parameters

* $\eta$ and $decay$: learning rate and decay rate of $\eta$, respectively.

* $velocity$: used in NAG.

* $\gamma$: used in exponentially weighted average, default value is $0.9$.

* $\beta_1$ and $\beta_2$: used in Adam based algorithm, default values $\beta_1 = 0.9$, $\beta_2 = 0.999$.

* $\epsilon$: very small value to prevent 0 as a denominator, like $10^{-8}$.

| Name     | Parameters                                        |
|----------|---------------------------------------------------|
| Adagrad  | $\eta$, $decay$, $\epsilon$                       |
| NAdagrad | $\eta$, $decay$, $velocity$, $\epsilon$           |
| Adadelta | $\gamma$, $\epsilon$                              |
| RMSProp  | $\eta$, $decay$, $\gamma$, $\epsilon$             |
| NRMSProp | $\eta$, $decay$, $\gamma$, $velocity$, $\epsilon$ |
| Adam     | $\eta$, $decay$, $\beta_1$, $\beta_2$, $\epsilon$ |
| NAdam    | $\eta$, $decay$, $\beta_1$, $\beta_2$, $\epsilon$ |
| AdaMax   | $\eta$, $decay$, $\beta_1$, $\beta_2$, $\epsilon$ |
| AMSGrad  | $\eta$, $decay$, $\beta_1$, $\beta_2$, $\epsilon$ |


# References
